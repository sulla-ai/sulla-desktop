export const ollamaLocalModels = `---
schemaversion: 1
slug: ollama-local-models
title: Ollama & Local Models
section: Getting Started
tags:
  - Models
  - local
order: 10
locked: false
author: seed
created_at: 2026-02-02T03:00:00Z
updated_at: 2026-02-02T03:00:00Z
---

## What is Ollama

Ollama runs LLMs locally so Sulla can operate with low latency and strong privacy properties.

## When to use local models

- Sensitive work
- Offline use
- Lower cost

## Tradeoffs

- Capability vs larger remote models
- Hardware constraints
- Model availability and tuning

## Typical workflow

- Choose a model
- Verify it runs locally
- Use it for planning/execution as appropriate
`;