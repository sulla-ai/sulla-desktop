// BaseNode - Abstract base class for all graph nodes
// Uses LLMServiceFactory for LLM interactions (supports both local Ollama and remote APIs)

import type { GraphNode, ThreadState, NodeResult, Message } from '../types';
import type { ILLMService } from '../services/ILLMService';
import type { ChatMessage } from '../services/ILLMService';
import { getLLMService, getCurrentMode } from '../services/LLMServiceFactory';
import { getOllamaService } from '../services/OllamaService';
import { getAwarenessService } from '../services/AwarenessService';
import { jsonrepair } from 'jsonrepair';

// Import soul.md via raw-loader (configured in vue.config.mjs)
// @ts-ignore - raw-loader import
import soulPromptRaw from '../prompts/soul.md';

function getSoulPrompt(): string {
  // raw-loader may return { default: string } or string depending on config
  const content = typeof soulPromptRaw === 'string' ? soulPromptRaw : soulPromptRaw.default;
  return (content || '').trim();
}

export interface LLMOptions {
  model?: string;
  maxTokens?: number;
  format?: 'json' | undefined;
}

export interface LLMResponse {
  content: string;
  model: string;
  evalCount?: number;
  evalDuration?: number;
}

export type ToolPromptDetail = 'names' | 'tactical' | 'planning';

export interface PromptEnrichmentOptions {
  includeSoul?: boolean;
  includeAwareness?: boolean;
  includeMemory?: boolean;
  includeConversation?: boolean;
  conversationOverride?: string;
  maxHistoryItems?: number;
  includeTools?: boolean;
  toolDetail?: ToolPromptDetail;
  includeSkills?: boolean;
  includeStrategicPlan?: boolean;
  includeTacticalPlan?: boolean;
  requireJson?: boolean;
}

export abstract class BaseNode implements GraphNode {
  id: string;
  name: string;
  protected availableModel: string | null = null;
  protected llmService: ILLMService | null = null;

  constructor(id: string, name: string) {
    this.id = id;
    this.name = name;
  }

  protected prefixWithSoulPrompt(prompt: string): string {
    const soulPrompt = getSoulPrompt();
    const soulHeader = `${soulPrompt}\n\n---\n\n`;
    return prompt.startsWith(soulHeader)
      ? prompt
      : `${soulHeader}${prompt}`;
  }

  protected async enrichPrompt(
    basePrompt: string,
    state: ThreadState,
    options: PromptEnrichmentOptions = {},
  ): Promise<string> {
    const parts: string[] = [];

    if (options.requireJson) {
      parts.push('CRITICAL: You MUST respond with ONLY valid JSON. No markdown, no explanation, no conversation. Start your response with { and end with }.');
    }

    if (options.includeAwareness) {
      try {
        const awareness = getAwarenessService();
        await awareness.initialize();
        const identity = awareness.identityPrompt;
        if (identity.trim()) {
          parts.push(`Awareness context:\n${identity}`);
        }
      } catch {
        // best effort
      }
    }

    if (options.includeMemory && state.metadata.memoryContext) {
      parts.push(`Relevant context from memory:\n${String(state.metadata.memoryContext)}`);
    }

    if (options.includeConversation) {
      const override = options.conversationOverride;
      if (override && override.trim()) {
        parts.push(`Conversation history:\n${override.trim()}`);
      } else {
        const maxItems = options.maxHistoryItems || 10;
        const historySource = state.messages.length > 0 ? state.messages : state.shortTermMemory;
        const userAssistantMessages = historySource.filter(m => m.role === 'user' || m.role === 'assistant');
        if (userAssistantMessages.length > 0) {
          const lines = userAssistantMessages
            .slice(-maxItems)
            .map(m => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`);
          parts.push(`Conversation history:\n${lines.join('\n')}`);
        }
      }
    }

    if (options.includeTools) {
      try {
        const { registerDefaultTools, getToolRegistry } = await import('../tools');
        registerDefaultTools();
        const registry = getToolRegistry();
        const detail: ToolPromptDetail = options.toolDetail || 'names';

        if (detail === 'tactical') {
          parts.push(`Available tools:\n${registry.getTacticalInstructionsBlock()}`);
        } else if (detail === 'planning') {
          const toolParts = registry.listUnique().map(t => t.getPlanningInstructions());
          parts.push(`Available tools:\n${toolParts.join('\n\n')}`);
        } else {
          const toolLines = registry.listUnique().map(t => `- ${t.name}`);
          parts.push(`Available tools:\n${toolLines.join('\n')}`);
        }
      } catch {
        // best effort
      }
    }

    if (options.includeSkills) {
      try {
        const { getSkillService } = await import('../services/SkillService');
        const skillService = getSkillService();
        await skillService.initialize();
        const enabledSkills = await skillService.listEnabledSkills();
        if (enabledSkills.length > 0) {
          const lines = enabledSkills.map(s => `- ${s.id}: ${s.description || s.title || ''}`);
          parts.push(`Enabled skills:\n${lines.join('\n')}`);
        }
      } catch {
        // best effort
      }
    }

    if (options.includeStrategicPlan || options.includeTacticalPlan) {
      const planBlock = this.buildPlanContextBlock(state, !!options.includeStrategicPlan, !!options.includeTacticalPlan);
      if (planBlock) {
        parts.push(planBlock);
      }
    }

    parts.push(basePrompt);

    const enriched = parts.join('\n\n');
    return options.includeSoul ? this.prefixWithSoulPrompt(enriched) : enriched;
  }

  protected extractFirstJSONObjectText(text: string): string | null {
    const src = String(text || '');
    const match = src.match(/\{[\s\S]*\}/);
    return match ? match[0] : null;
  }

  protected parseJsonLenient<T = unknown>(text: string): T | null {
    try {
      return JSON.parse(text) as T;
    } catch {
      try {
        const repaired = jsonrepair(text);
        return JSON.parse(repaired) as T;
      } catch {
        return null;
      }
    }
  }

  protected parseFirstJSONObject<T = unknown>(text: string): T | null {
    const jsonText = this.extractFirstJSONObjectText(text);
    if (!jsonText) {
      return null;
    }
    return this.parseJsonLenient<T>(jsonText);
  }

  /**
   * Build a context block showing strategic and tactical plan progress
   * @param strategicOnly If true, only include strategic plan (no tactical steps)
   */
  protected buildPlanContextBlock(state: ThreadState, includeStrategic: boolean, includeTactical: boolean): string | null {
    const strategicPlan = state.metadata.strategicPlan as {
      goal?: string;
      milestones?: Array<{ id: string; title: string; description?: string; status?: string }>;
    } | undefined;

    const tacticalPlan = state.metadata.tacticalPlan as {
      milestoneId?: string;
      steps?: Array<{ id: string; action: string; description?: string; status?: string }>;
    } | undefined;

    const parts: string[] = [];

    // Strategic plan milestones
    if (includeStrategic && strategicPlan?.milestones && strategicPlan.milestones.length > 0) {
      const milestoneLines = strategicPlan.milestones.map(m => {
        const status = m.status || 'pending';
        const statusIcon = status === 'completed' ? '✓' : status === 'in_progress' ? '→' : status === 'failed' ? '✗' : '○';
        const title = status === 'in_progress' ? `**${m.title}**` : m.title;
        return `  ${statusIcon} ${title} [${status}]`;
      });
      parts.push(`## Strategic Plan\nGoal: ${strategicPlan.goal || 'Not specified'}\n\nMilestones:\n${milestoneLines.join('\n')}`);
    }

    // Tactical plan steps for current milestone
    if (includeTactical && tacticalPlan?.steps && tacticalPlan.steps.length > 0) {
      const stepLines = tacticalPlan.steps.map((s, idx) => {
        const status = s.status || 'pending';
        const statusIcon = status === 'done' ? '✓' : status === 'in_progress' ? '→' : status === 'failed' ? '✗' : '○';
        const action = status === 'in_progress' ? `**${s.action}**` : s.action;
        return `  ${statusIcon} Step ${idx + 1}: ${action} [${status}]`;
      });
      parts.push(`## Tactical Plan (Current Milestone)\nSteps:\n${stepLines.join('\n')}`);
    }

    return parts.length > 0 ? parts.join('\n\n') : null;
  }

  private createInternalMessageId(prefix: string): string {
    return `${prefix}_${Date.now()}_${Math.floor(Math.random() * 1_000_000)}`;
  }

  private toChatMessagesFromThread(state: ThreadState): ChatMessage[] {
    const out: ChatMessage[] = [];
    for (const m of state.messages) {
      if (m.role !== 'user' && m.role !== 'assistant' && m.role !== 'system') {
        continue;
      }
      out.push({ role: m.role, content: m.content });
    }
    return out;
  }

  abstract execute(state: ThreadState): Promise<{ state: ThreadState; next: NodeResult }>;

  async initialize(): Promise<void> {
    console.log(`[Agent:${this.name}] Initializing...`);

    // Get the appropriate LLM service (local or remote)
    this.llmService = getLLMService();
    await this.llmService.initialize();

    this.availableModel = this.llmService.getModel();
    const mode = getCurrentMode();

    console.log(`[Agent:${this.name}] Mode: ${mode}, Model: ${this.availableModel || 'none'}`);
  }

  async destroy(): Promise<void> {
    this.llmService = null;
  }

  /**
   * Pull a model from Ollama (only works for local mode)
   */
  protected async pullModel(modelName: string): Promise<boolean> {
    if (getCurrentMode() !== 'local') {
      console.warn(`[Agent:${this.name}] Cannot pull model in remote mode`);

      return false;
    }

    const ollama = getOllamaService();

    return ollama.pullModel(modelName);
  }

  /**
   * Send a prompt to the LLM (works for both local and remote)
   * Falls back to local Ollama if remote service is unavailable
   * If state contains heartbeatModel override, uses that model instead
   */
  protected async prompt(prompt: string, state?: ThreadState, storeInMessages = true): Promise<LLMResponse | null> {
    // Check for heartbeat model override in state metadata
    const heartbeatModel = state?.metadata?.heartbeatModel as string | undefined;
    
    if (heartbeatModel && heartbeatModel !== 'default') {
      return this.promptWithModelOverride(prompt, heartbeatModel, state);
    }

    if (!this.llmService) {
      this.llmService = getLLMService();
      await this.llmService.initialize();
      this.availableModel = this.llmService.getModel();
    }

    // If primary service is not available, try fallback to Ollama
    if (!this.llmService.isAvailable()) {
      console.warn(`[Agent:${this.name}] Primary LLM service not available, attempting fallback to Ollama`);
      
      try {
        const ollama = getOllamaService();
        await ollama.initialize();
        
        if (ollama.isAvailable()) {
          console.log(`[Agent:${this.name}] Falling back to local Ollama`);
          this.llmService = ollama;
          this.availableModel = ollama.getModel();
        } else {
          console.error(`[Agent:${this.name}] Ollama fallback also not available`);
          return null;
        }
      } catch (err) {
        console.error(`[Agent:${this.name}] Fallback to Ollama failed:`, err);
        return null;
      }
    }

    const model = this.availableModel || this.llmService.getModel();

    try {
      let messages: ChatMessage[] = [{ role: 'user', content: prompt }];
      if (state) {
        if (storeInMessages) {
          const promptMsg: Message = {
            id: this.createInternalMessageId('node_prompt'),
            role: 'system',
            content: prompt,
            timestamp: Date.now(),
            metadata: { nodeId: this.id, nodeName: this.name, kind: 'node_prompt' },
          };
          state.messages.push(promptMsg);
        }
        messages = this.toChatMessagesFromThread(state);
      }

      const content = await this.llmService.chat(messages);

      if (!content) {
        console.warn(`[Agent:${this.name}] No response from LLM`);

        return null;
      }

      if (state && storeInMessages) {
        const responseMsg: Message = {
          id: this.createInternalMessageId('node_response'),
          role: 'assistant',
          content,
          timestamp: Date.now(),
          metadata: { nodeId: this.id, nodeName: this.name, kind: 'node_response', model },
        };
        state.messages.push(responseMsg);
      }

      const result = {
        content,
        model,
      };

      console.log(`[Agent:${this.name}] Response received (${result.content.length} chars)`);

      return result;
    } catch (err) {
      console.error(`[Agent:${this.name}] Prompt failed:`, err);

      return null;
    }
  }

  /**
   * Send a prompt with a specific model override (for heartbeat)
   * Format: 'local:modelname' or 'remote:provider:model'
   */
  private async promptWithModelOverride(prompt: string, modelOverride: string, state?: ThreadState): Promise<LLMResponse | null> {
    console.log(`[Agent:${this.name}] Using model override: ${modelOverride}`);
    
    try {
      if (modelOverride.startsWith('local:')) {
        // Local Ollama model
        const modelName = modelOverride.substring(6); // Remove 'local:' prefix
        const ollama = getOllamaService();
        await ollama.initialize();
        
        if (!ollama.isAvailable()) {
          console.error(`[Agent:${this.name}] Ollama not available for model override`);
          return null;
        }
        
        let messages: ChatMessage[] = [{ role: 'user', content: prompt }];
        if (state) {
          const promptMsg: Message = {
            id: this.createInternalMessageId('node_prompt'),
            role: 'system',
            content: prompt,
            timestamp: Date.now(),
            metadata: { nodeId: this.id, nodeName: this.name, kind: 'node_prompt', modelOverride },
          };
          state.messages.push(promptMsg);
          messages = this.toChatMessagesFromThread(state);
        }

        const content = await ollama.chatWithModel(messages, modelName);
        
        if (!content) {
          return null;
        }
        
        if (state) {
          const responseMsg: Message = {
            id: this.createInternalMessageId('node_response'),
            role: 'assistant',
            content,
            timestamp: Date.now(),
            metadata: { nodeId: this.id, nodeName: this.name, kind: 'node_response', model: modelName, modelOverride },
          };
          state.messages.push(responseMsg);
        }

        return { content, model: modelName };
      } else if (modelOverride.startsWith('remote:')) {
        // Remote model: format is 'remote:provider:model'
        const parts = modelOverride.substring(7).split(':'); // Remove 'remote:' prefix
        if (parts.length < 2) {
          console.error(`[Agent:${this.name}] Invalid remote model format: ${modelOverride}`);
          return null;
        }
        
        const [provider, ...modelParts] = parts;
        const modelName = modelParts.join(':');
        
        // Import and use RemoteModelService with override
        const { getRemoteModelService } = await import('../services/RemoteModelService');
        const remoteService = getRemoteModelService();
        await remoteService.initialize();
        
        if (!remoteService.isAvailable()) {
          console.error(`[Agent:${this.name}] Remote service not available for model override`);
          return null;
        }
        
        // Use the remote service with the specific model
        // Remote override API currently accepts a single prompt string.
        // When state is provided, serialize the accumulated thread into a single prompt.
        const effectivePrompt = state
          ? this.toChatMessagesFromThread(state).map(m => `${m.role}: ${m.content}`).join('\n\n')
          : prompt;

        const content = await remoteService.generateWithModel(effectivePrompt, provider, modelName);
        
        if (!content) {
          return null;
        }
        
        if (state) {
          const responseMsg: Message = {
            id: this.createInternalMessageId('node_response'),
            role: 'assistant',
            content,
            timestamp: Date.now(),
            metadata: { nodeId: this.id, nodeName: this.name, kind: 'node_response', model: `${provider}:${modelName}`, modelOverride },
          };
          state.messages.push(responseMsg);
        }

        return { content, model: `${provider}:${modelName}` };
      } else {
        console.error(`[Agent:${this.name}] Unknown model override format: ${modelOverride}`);
        return null;
      }
    } catch (err) {
      console.error(`[Agent:${this.name}] Model override prompt failed:`, err);
      return null;
    }
  }

  /**
   * Send a prompt and parse JSON response
   */
  protected async promptJSON<T = unknown>(prompt: string, state?: ThreadState): Promise<T | null> {
    const response = await this.prompt(prompt, state);

    if (!response) {
      return null;
    }

    try {
      return JSON.parse(response.content) as T;
    } catch {
      return null;
    }
  }

  /**
   * Build a prompt with context from thread state
   */
  protected buildContextualPrompt(
    instruction: string,
    state: ThreadState,
    options: { includeMemory?: boolean; includeHistory?: boolean; maxHistoryItems?: number } = {},
  ): string {
    const parts: string[] = [];

    parts.push(getAwarenessService().identityPrompt);
    parts.push('');

    const historyForLog: Array<{ role: string; content: string }> = [];
    if (options.includeHistory && state.shortTermMemory.length > 0) {
      const maxItems = options.maxHistoryItems || 6;
      historyForLog.push(...state.shortTermMemory.slice(-maxItems));
    }

    // Add memory context if available and requested
    if (options.includeMemory && state.metadata.memoryContext) {
      parts.push('Relevant context from memory:');
      parts.push(state.metadata.memoryContext as string);
      parts.push('');
    }

    // Add conversation history if requested
    // Use full messages array for better continuity, fall back to shortTermMemory
    // Filter out system messages to avoid confusing the LLM with prior prompt context
    if (options.includeHistory) {
      const maxItems = options.maxHistoryItems || 10;
      const historySource = state.messages.length > 0 ? state.messages : state.shortTermMemory;
      const userAssistantMessages = historySource.filter(m => m.role === 'user' || m.role === 'assistant');

      if (userAssistantMessages.length > 0) {
        parts.push('Conversation history:');
        userAssistantMessages.slice(-maxItems).forEach(m => {
          parts.push(`${ m.role === 'user' ? 'User' : 'Assistant' }: ${ m.content }`);
        });
        parts.push('');
      }
    }

    // Add the main instruction
    parts.push(instruction);

    const finalPrompt = parts.join('\n');

    return finalPrompt;
  }
}
